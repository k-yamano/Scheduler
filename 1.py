import os, re, glob, time, random, io
import numpy as np
import pandas as pd
from datetime import datetime

# --- 1. Google Colabèªè¨¼ï¼ˆ1å›ã®ã¿ï¼‰ ---
print("=" * 60)
print("Google Colab - ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›´æ–°ãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
print("=" * 60)

print("\n[1/9] èªè¨¼å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
from google.colab import auth
import gspread
from google.auth import default
from googleapiclient.discovery import build

# Googleèªè¨¼ï¼ˆã“ã®1å›ã ã‘ï¼‰
auth.authenticate_user()
print("  âœ“ Googleèªè¨¼å®Œäº†")

# èªè¨¼æƒ…å ±å–å¾—
creds, _ = default()

# Google Sheets APIæ¥ç¶š
gc = gspread.authorize(creds)
SHEET_KEY = "1g3ZeCFzexguuu6q3r7kS3tOHqq44JtDarnnwd8wpRhc"
sh = gc.open_by_key(SHEET_KEY)
print(f"  âœ“ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ¥ç¶šå®Œäº†")

# Google Drive APIæ¥ç¶šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Šç”¨ï¼‰
drive_service = build('drive', 'v3', credentials=creds)
print(f"  âœ“ Google Drive APIæ¥ç¶šå®Œäº†")

# --- 2. Google Driveãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢é–¢æ•° ---
def search_drive_files(query, service):
    """Google Drive APIã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
    try:
        results = service.files().list(
            q=query,
            spaces='drive',
            fields='files(id, name, mimeType)',
            pageSize=10
        ).execute()
        return results.get('files', [])
    except Exception as e:
        print(f"    æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def download_file(file_id, service):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        request = service.files().get_media(fileId=file_id)
        file_content = io.BytesIO()
        from googleapiclient.http import MediaIoBaseDownload
        downloader = MediaIoBaseDownload(file_content, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
        file_content.seek(0)
        return file_content
    except Exception as e:
        print(f" ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- 3. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---
def _retry_google(func, *args, retries=5, **kwargs):
    """APIã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒªãƒˆãƒ©ã‚¤å‡¦ç†"""
    for i in range(retries):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            s = str(e)
            if "429" in s or "500" in s or "503" in s:
                wait = (2 ** i) + random.random()
                print(f"    â³ APIã‚¨ãƒ©ãƒ¼ã€{wait:.1f}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
                time.sleep(wait)
                continue
            raise
    return func(*args, **kwargs)

def read_csv_flexible(file_obj):
    """è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œã—ã¦CSVèª­ã¿è¾¼ã¿"""
    for enc in ["utf-8-sig", "cp932", "utf-8", "shift_jis"]:
        try:
            file_obj.seek(0)
            return pd.read_csv(file_obj, encoding=enc)
        except:
            pass
    try:
        file_obj.seek(0)
        return pd.read_csv(file_obj, sep="\t")
    except Exception as e:
        raise Exception(f"èª­è¾¼å¤±æ•—: ({e})")

def parse_last_number(x):
    """æ–‡å­—åˆ—ã‹ã‚‰æœ€å¾Œã®æ•°å€¤ã‚’æŠ½å‡º"""
    if pd.isna(x):
        return np.nan
    m = re.findall(r'\d+(?:\.\d+)?', str(x).replace(',', ''))
    return float(m[-1]) if m else np.nan

def pretty_num(x):
    """æ•°å€¤ã‚’æ•´å½¢ï¼ˆæ•´æ•°ã¯å°æ•°ç‚¹ãªã—ã€å°æ•°ã¯2æ¡ï¼‰"""
    if pd.isna(x) or x == "":
        return ""
    try:
        f = float(x)
        return int(f) if f.is_integer() else round(f, 2)
    except:
        return x

# --- 4. ãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢ ---
print("\n[2/9] Google Driveã‹ã‚‰ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢ä¸­...")

files_found = {}

def find_drive_file(key, patterns):
    """ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
    for pattern in patterns:
        # ãƒ•ã‚¡ã‚¤ãƒ«åã§æ¤œç´¢
        query = f"name contains '{pattern.replace('*', '')}' and trashed=false"
        results = search_drive_files(query, drive_service)

        if results:
            file = results[0]
            files_found[key] = file
            print(f"  âœ“ {key:12s}: {file['name']}")
            return

    print(f"{key:12s}: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

find_drive_file("product", ['product.csv', 'è£½å“.csv', 'è£½å“.xlsx'])
find_drive_file("zaiko", ['zaiko.xlsx', 'åœ¨åº«.xlsx'])
find_drive_file("honpo", ['éœ€è¦äºˆæ¸¬_æœ¬èˆ—.csv', 'æœ¬èˆ—.csv'])
find_drive_file("sales", ['éœ€è¦äºˆæ¸¬_è²©å£².csv', 'è²©å£².csv'])
find_drive_file("workday", ['workday.csv', 'ç¨¼åƒæ—¥.csv', 'å–¶æ¥­æ—¥.csv'])
find_drive_file("base", ['base_file.csv', 'base.csv'])

# --- 5. ç¨¼åƒæ—¥æ•°è¨ˆç®— ---
print("\n[3/9] ç¨¼åƒæ—¥æ•°ã‚’è¨ˆç®—ä¸­...")

today = pd.Timestamp.now(tz='Asia/Tokyo')
FY_ORDER = ['4æœˆ','5æœˆ','6æœˆ','7æœˆ','8æœˆ','9æœˆ','10æœˆ','11æœˆ','12æœˆ','1æœˆ','2æœˆ','3æœˆ']
WINDOW_LABELS = [FY_ORDER[(FY_ORDER.index(f"{today.month}æœˆ") + k) % 12] for k in range(4)]
print(f"  å¯¾è±¡æœˆ: {' â†’ '.join(WINDOW_LABELS)}")

def get_workdays(file_info, year, month):
    """ç¨¼åƒæ—¥æ•°ã‚’å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯å–¶æ¥­æ—¥è¨ˆç®—ï¼‰"""
    if not file_info:
        s = pd.Timestamp(year, month, 1)
        e = s + pd.offsets.MonthEnd(1)
        return float(len(pd.date_range(start=s, end=e, freq="B")))

    try:
        file_content = download_file(file_info['id'], drive_service)
        if not file_content:
            raise Exception("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")

        df = read_csv_flexible(file_content).fillna("")
        df.columns = [str(c).strip() for c in df.columns]
        cur_ym, cur_m = f"{year}-{month:02d}", f"{month}æœˆ"

        for mk in df.columns:
            if any(k in mk for k in ["å¹´æœˆ","æœˆ"]):
                for wk in df.columns:
                    if any(k in wk for k in ["ç¨¼åƒæ—¥","å–¶æ¥­æ—¥"]):
                        hit = df[df[mk].astype(str).str.contains(cur_ym)|df[mk].astype(str).str.contains(cur_m)]
                        if not hit.empty:
                            v = pd.to_numeric(hit[wk], errors='coerce').dropna()
                            if not v.empty:
                                return float(v.iloc[0])
    except:
        pass

    s = pd.Timestamp(year, month, 1)
    e = s + pd.offsets.MonthEnd(1)
    return float(len(pd.date_range(start=s, end=e, freq="B")))

workdays_map = {}
for k in range(4):
    dt = (today.tz_localize(None) + pd.DateOffset(months=k))
    label = WINDOW_LABELS[k]
    workdays_map[label] = get_workdays(files_found.get("workday"), dt.year, dt.month)
    print(f"  {dt.year}å¹´{dt.month:02d}æœˆ ({label}): {workdays_map[label]:.0f}æ—¥")

# --- 6. ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
print("\n[4/9] ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")

# è£½å“ãƒã‚¹ã‚¿
prod_info = files_found.get("product")
if not prod_info:
    raise FileNotFoundError("è£½å“ãƒã‚¹ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

prod_content = download_file(prod_info['id'], drive_service)
if prod_info['name'].endswith(('.xlsx', '.xls')):
    df_prod = pd.read_excel(prod_content)
else:
    df_prod = read_csv_flexible(prod_content)

df_prod.columns = [str(c).strip() for c in df_prod.columns]
df_prod = df_prod.rename(columns={
    df_prod.columns[0]: "å“ç•ª",
    df_prod.columns[1]: "å•†å“å"
})

if len(df_prod.columns) >= 3:
    df_prod = df_prod.rename(columns={df_prod.columns[2]: "ç™ºæ³¨ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ "})
else:
    df_prod["ç™ºæ³¨ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ "] = 0

df_prod["ç™ºæ³¨ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ "] = pd.to_numeric(df_prod["ç™ºæ³¨ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ "], errors='coerce').fillna(0)
df_prod["å“ç•ª"] = df_prod["å“ç•ª"].astype(str).str.strip()
print(f"  âœ“ è£½å“ãƒã‚¹ã‚¿: {len(df_prod):,}ä»¶")

# åœ¨åº«ãƒ‡ãƒ¼ã‚¿
zaiko_info = files_found.get("zaiko")
if not zaiko_info:
    raise FileNotFoundError("åœ¨åº«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

zaiko_content = download_file(zaiko_info['id'], drive_service)
df_zaiko_raw = pd.read_excel(zaiko_content, sheet_name=0, header=None)
df_zaiko = df_zaiko_raw.iloc[:, [1, 3]].copy()
df_zaiko.columns = ["å“ç•ª", "åœ¨åº«æ•°é‡"]
df_zaiko["å“ç•ª"] = df_zaiko["å“ç•ª"].astype(str).str.strip()
df_zaiko["åœ¨åº«æ•°é‡"] = pd.to_numeric(df_zaiko["åœ¨åº«æ•°é‡"], errors='coerce').fillna(0)
df_zaiko = df_zaiko.groupby("å“ç•ª", as_index=False)["åœ¨åº«æ•°é‡"].sum()
print(f"  âœ“ åœ¨åº«ãƒ‡ãƒ¼ã‚¿: {len(df_zaiko):,}ä»¶")

# --- 7. éœ€è¦äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ ---
print("\n[5/9] éœ€è¦äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ä¸­...")

def read_forecast(file_info, window_labels):
    """éœ€è¦äºˆæ¸¬CSVã‚’èª­ã¿è¾¼ã¿"""
    if not file_info:
        return pd.DataFrame({"å“ç•ª": []})

    try:
        file_content = download_file(file_info['id'], drive_service)
        df = read_csv_flexible(file_content)
        df["å“ç•ª"] = df["å“ç•ª"].astype(str).str.strip()

        for m in window_labels:
            if m not in df.columns:
                df[m] = np.nan
            else:
                df[m] = df[m].apply(parse_last_number)

        return df[["å“ç•ª"] + list(window_labels)]
    except:
        return pd.DataFrame({"å“ç•ª": []})

df_need = pd.merge(
    read_forecast(files_found.get("honpo"), WINDOW_LABELS),
    read_forecast(files_found.get("sales"), WINDOW_LABELS),
    on="å“ç•ª",
    how="outer",
    suffixes=("_æœ¬èˆ—", "_è²©å£²")
)

for m in WINDOW_LABELS:
    df_need[m] = df_need.filter(regex=f"^{m}").sum(axis=1, skipna=True)
    wd = workdays_map[m]
    df_need[f"{m}æ—¥å‰²"] = (df_need[m] / wd) if wd else np.nan

print(f"  âœ“ éœ€è¦äºˆæ¸¬: {len(df_need):,}ä»¶")

# --- 8. ç§»å‹•å¹³å‡ãƒ»å®‰å…¨åœ¨åº« ---
print("\n[6/9] ç§»å‹•å¹³å‡ãƒ»å®‰å…¨åœ¨åº«ã‚’è¨ˆç®—ä¸­...")

base_info = files_found.get("base")
if not base_info:
    raise FileNotFoundError("Baseãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

base_content = download_file(base_info['id'], drive_service)
df_base = read_csv_flexible(base_content)

colA, colB, colS = df_base.columns[0], df_base.columns[1], df_base.columns[18]
df_base[colA] = pd.to_datetime(df_base[colA], errors="coerce")
df_base[colB] = df_base[colB].astype(str).str.strip()

start = (today - pd.DateOffset(months=3)).tz_localize(None)
df_base3 = df_base[df_base[colA] >= start]

df_ma = df_base3.groupby(colB, as_index=False)[colS].mean().rename(columns={colB: "å“ç•ª", colS: "ç§»å‹•å¹³å‡"})
df_std = df_base3.groupby(colB, as_index=False)[colS].std().rename(columns={colB: "å“ç•ª", colS: "ä½¿ç”¨é‡æ¨™æº–åå·®"}).fillna(0)
df_stats = pd.merge(df_ma, df_std, on="å“ç•ª", how="left")

def calc_safety(std, lead, interval=7, factor=1.65):
    """å®‰å…¨åœ¨åº«ã‚’è¨ˆç®—"""
    import math
    if pd.isna(std) or pd.isna(lead) or std == 0:
        return 0
    return factor * std * math.sqrt(lead + interval)

print(f"  âœ“ çµ±è¨ˆãƒ‡ãƒ¼ã‚¿: {len(df_stats):,}ä»¶")

# --- 9. Google Sheetså‡ºåŠ› ---
print("\n[7/9] Google Sheetsã¸å‡ºåŠ›ä¸­...")

def get_or_create_worksheet(spreadsheet, sheet_name, rows=1000, cols=20):
    """ã‚·ãƒ¼ãƒˆã‚’å–å¾—ã¾ãŸã¯ã‚³ãƒ”ãƒ¼ä½œæˆ"""
    try:
        ws = spreadsheet.worksheet(sheet_name)
        print(f"    âœ“ '{sheet_name}' (æ—¢å­˜)")
        return ws
    except gspread.exceptions.WorksheetNotFound:
        try:
            template = spreadsheet.worksheet("format")
            new_ws = _retry_google(
                spreadsheet.duplicate_sheet,
                source_sheet_id=template.id,
                new_sheet_name=sheet_name
            )
            print(f"    âœ“ '{sheet_name}' (formatã‹ã‚‰ã‚³ãƒ”ãƒ¼)")
            return new_ws
        except gspread.exceptions.WorksheetNotFound:
            ws = _retry_google(spreadsheet.add_worksheet, title=sheet_name, rows=rows, cols=cols)
            print(f"    âœ“ '{sheet_name}' (æ–°è¦ä½œæˆ)")
            return ws

month_sheet_names = [(today + pd.DateOffset(months=k)).strftime("%Y/%m") for k in range(4)]
all_merge_requests = []

for k, sheet_name in enumerate(month_sheet_names):
    month_label = WINDOW_LABELS[k]
    col = f"{month_label}æ—¥å‰²"

    # ãƒ‡ãƒ¼ã‚¿çµåˆ
    df_out = (df_prod
        .merge(df_zaiko, on="å“ç•ª", how="left")
        .merge(df_need[["å“ç•ª", col]], on="å“ç•ª", how="left")
        .merge(df_stats, on="å“ç•ª", how="left"))

    df_out = df_out.rename(columns={col: "æ—¥å‰²"})
    df_out["å®‰å…¨åœ¨åº«"] = df_out.apply(lambda r: calc_safety(r["ä½¿ç”¨é‡æ¨™æº–åå·®"], r["ç™ºæ³¨ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ "]), axis=1)

    for c in ["åœ¨åº«æ•°é‡", "æ—¥å‰²", "ç§»å‹•å¹³å‡", "å®‰å…¨åœ¨åº«"]:
        df_out[c] = df_out[c].map(pretty_num)

    df_out = df_out[["å“ç•ª", "å•†å“å", "åœ¨åº«æ•°é‡", "æ—¥å‰²", "ç§»å‹•å¹³å‡", "å®‰å…¨åœ¨åº«"]]

    # ã‚·ãƒ¼ãƒˆæº–å‚™
    required_rows = len(df_out) * 2 + 10
    ws = get_or_create_worksheet(sh, sheet_name, rows=required_rows, cols=20)

    # ã‚¯ãƒªã‚¢
    max_rows_to_clear = max(required_rows, 500)
    try:
        _retry_google(ws.clear_basic_filter)
    except:
        pass

    _retry_google(ws.batch_clear, [f"A3:F{max_rows_to_clear}"])

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆ1è¡ŒãŠãã«ç©ºè¡Œï¼‰
    header_row = [df_out.columns.tolist()]
    data_rows_interleaved = []
    merge_requests_for_this_sheet = []

    start_row = 4
    num_cols = len(df_out.columns)
    df_out_filled = df_out.fillna("")

    if len(df_out) > 0:
        for i, row in enumerate(df_out_filled.values.tolist()):
            data_rows_interleaved.append(row)
            data_rows_interleaved.append([""] * num_cols)

            current_data_row_gspread = start_row + (i * 2)
            current_data_row_api = current_data_row_gspread - 1

            # ã‚»ãƒ«çµåˆãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆAï½Fåˆ—ï¼‰
            for j in range(num_cols):
                merge_requests_for_this_sheet.append({
                    "mergeCells": {
                        "range": {
                            "sheetId": ws.id,
                            "startRowIndex": current_data_row_api,
                            "endRowIndex": current_data_row_api + 2,
                            "startColumnIndex": j,
                            "endColumnIndex": j + 1
                        },
                        "mergeType": "MERGE_ALL"
                    }
                })

    # ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
    data_to_write = header_row + data_rows_interleaved

    if data_to_write:
        end_row = 3 + len(data_to_write) - 1
        write_range = f"A3:F{end_row}"

        _retry_google(
            ws.update,
            range_name=write_range,
            values=data_to_write,
            value_input_option='USER_ENTERED'
        )
        print(f"{len(df_out):,}è¡Œæ›¸ãè¾¼ã¿å®Œäº†")

        all_merge_requests.extend(merge_requests_for_this_sheet)

# ä¸€æ‹¬ã‚»ãƒ«çµåˆ
if all_merge_requests:
    print("\n[8/9] ã‚»ãƒ«çµåˆã‚’å®Ÿè¡Œä¸­...")
    try:
        body = {"requests": all_merge_requests}
        _retry_google(sh.batch_update, body)
        print(f"  âœ“ {len(all_merge_requests):,}ä»¶ã®çµåˆå®Œäº†")
    except Exception as e:
        print(f"çµåˆã‚¨ãƒ©ãƒ¼: {e}")

# --- 10. ã‚·ãƒ¼ãƒˆä¸¦ã¹æ›¿ãˆ ---
print("\n[9/9] ã‚·ãƒ¼ãƒˆã‚’æœˆé †ã«ä¸¦ã¹æ›¿ãˆä¸­...")

all_worksheets = _retry_google(sh.worksheets)
sheet_order = {}

for ws in all_worksheets:
    name = ws.title
    if re.match(r'^\d{4}/\d{2}$', name):
        try:
            date_obj = datetime.strptime(name, "%Y/%m")
            sheet_order[name] = date_obj
        except:
            pass

sorted_sheets = sorted(sheet_order.items(), key=lambda x: x[1])

for idx, (sheet_name, _) in enumerate(sorted_sheets):
    try:
        ws = sh.worksheet(sheet_name)
        _retry_google(ws.update_index, idx)
        print(f"  {sheet_name} â†’ ä½ç½® {idx + 1}")
    except Exception as e:
        print(f"{sheet_name}: {e}")

# --- 11. å®Œäº† ---
print("\n" + "=" * 60)
print("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
print("=" * 60)
print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
print("1. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ã:")
print(f"   https://docs.google.com/spreadsheets/d/{SHEET_KEY}/")
print("\n2. ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€ŒğŸ“… ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç®¡ç†ã€â†’ã€ŒğŸ”„ ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼æ›´æ–°ã€ã‚’å®Ÿè¡Œ")
print("\nâ€» Gåˆ—ä»¥é™ã«ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼æƒ…å ±ãŒè¿½åŠ ã•ã‚Œã¾ã™")
print("=" * 60)